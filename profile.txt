
Google Bert
bert + downstream task || tensorflow: https://github.com/google-research/bert 
bert + downstream task || pytorch:    https://github.com/codertimo/BERT-pytorch

DOCUMENT IMAGE UNDERSTANDING
text + image_layout bert || pytorch: https://github.com/microsoft/unilm/tree/master/layoutlm

attention-ocr
https://github.com/da03/Attention-OCR

==================================================================================================

CTC-单行图片文字识别
https://www.cnblogs.com/liaohuiqiang/p/9953978.html
https://zhuanlan.zhihu.com/p/43534801
http://nooverfit.com/wp/ctc%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%EF%BC%88connectionist-temporal-classification%E8%BF%9E%E6%8E%A5%E6%97%B6%E5%BA%8F%E5%88%86%E7%B1%BB%EF%BC%89%EF%BC%8C%E5%8D%95%E8%A1%8C%E6%96%87/

GNN综述
https://zhuanlan.zhihu.com/p/76001080
https://www.bilibili.com/video/BV1j54y1975p?spm_id_from=333.788.b_636f6d6d656e74.6  ||  微软课程
https://www.bilibili.com/video/BV1nQ4y1K7ze?spm_id_from=333.788.b_636f6d6d656e74.7  ||  Stanford课程

bert代码解读
https://blog.csdn.net/cpluss/article/details/88418176
https://www.jiqizhixin.com/articles/2018-11-01-9
https://zhuanlan.zhihu.com/p/54885596
http://fancyerii.github.io/2019/03/09/bert-codes/

self-attention理解
https://blog.csdn.net/cpluss/article/details/85330256

CNN参数值推导(in-channel(R/G/B可看作独立特征,类似文本的text-emb/token-type-emb/pos-emb,后续sum)、out-channel(kernels))
https://blog.csdn.net/cpluss/article/details/81709998

wide&deep讲解: 记忆能力(LR/FTRL)+泛化能力(DNN/AdaGrad)+logisticloss
https://www.jianshu.com/p/71cf3d1f579d
https://zhuanlan.zhihu.com/p/53361519

EM算法解读
https://blog.csdn.net/cpluss/article/details/88817337

CRF解读
https://blog.csdn.net/cpluss/article/details/88824303 ; https://blog.csdn.net/cpluss/article/details/88825748

==================================================================================================
