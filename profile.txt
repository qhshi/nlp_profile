
【Google Bert】
bert + downstream task || tensorflow: https://github.com/google-research/bert 
bert                   || pytorch:    https://github.com/codertimo/BERT-pytorch
bert pretrained models || pytorch:    https://github.com/huggingface/pytorch-pretrained-BERT

【DOCUMENT IMAGE UNDERSTANDING】
text + image_layout bert || pytorch: https://github.com/microsoft/unilm/tree/master/layoutlm

【attention-ocr】
https://github.com/da03/Attention-OCR

【Fast.ai库是一个新手友好型的深度学习工具箱，而且是目前复现最新算法的首要之选】
https://github.com/fastai/fastai

【称霸Kaggle的十大深度学习技巧】
https://blog.csdn.net/weixin_42137700/article/details/81529789

【keras版bert应用】
https://github.com/bojone/bert4keras/tree/master/examples

【nlp模型评价体系ACL20 Best Paper】
https://mp.weixin.qq.com/s/yGR3-tH2eAP__U58xZO9Rw
论文作者视频解析：
https://slideslive.com/38929272/beyond-accuracy-behavioral-testing-of-nlp-models-with-checklist
Github:
https://github.com/marcotcr/checklist
论文链接:
https://arxiv.org/abs/2005.04118

【xxx】


==================================================================================================

CTC-单行图片文字识别
https://www.cnblogs.com/liaohuiqiang/p/9953978.html
https://zhuanlan.zhihu.com/p/43534801
http://nooverfit.com/wp/ctc%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%EF%BC%88connectionist-temporal-classification%E8%BF%9E%E6%8E%A5%E6%97%B6%E5%BA%8F%E5%88%86%E7%B1%BB%EF%BC%89%EF%BC%8C%E5%8D%95%E8%A1%8C%E6%96%87/

GNN综述
https://zhuanlan.zhihu.com/p/76001080
https://www.bilibili.com/video/BV1j54y1975p?spm_id_from=333.788.b_636f6d6d656e74.6  ||  微软课程
https://www.bilibili.com/video/BV1nQ4y1K7ze?spm_id_from=333.788.b_636f6d6d656e74.7  ||  Stanford课程

bert代码解读
https://blog.csdn.net/cpluss/article/details/88418176
https://www.jiqizhixin.com/articles/2018-11-01-9
https://zhuanlan.zhihu.com/p/54885596
http://fancyerii.github.io/2019/03/09/bert-codes/

self-attention理解
https://blog.csdn.net/cpluss/article/details/85330256

CNN参数值推导(in-channel(R/G/B可看作独立特征,类似文本的text-emb/token-type-emb/pos-emb,后续sum)、out-channel(kernels))
https://blog.csdn.net/cpluss/article/details/81709998

wide&deep讲解: 记忆能力(LR/FTRL)+泛化能力(DNN/AdaGrad)+logisticloss
https://www.jianshu.com/p/71cf3d1f579d
https://zhuanlan.zhihu.com/p/53361519

EM算法解读
https://blog.csdn.net/cpluss/article/details/88817337

CRF解读
https://blog.csdn.net/cpluss/article/details/88824303 ; https://blog.csdn.net/cpluss/article/details/88825748

==================================================================================================
